<!DOCTYPE html>
<html lang="en-GB">
<head>
    <link rel="stylesheet" href="style.css" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=ABeeZee' rel='stylesheet' type="text/css">

    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width">

    <title>SATO Report</title>
</head>

<body prefix="schema: http://schema.org/ xsd: http://www.w3.org/2001/XMLSchema# sa: https://ns.science.ai/">
<header>
    <p class="title">Webdev Smart Aid Tool (SATO)</p>
</header>

<fieldset>
    <article typeof="schema:ScholarlyArticle" resource="#">
        <h1>SATO Documentation</h1>
        <section>
            <ol class="inline-list">
                <li class="inline-list-item" property="schema:author" typeof="sa:ContributorRole">
                    <a property="schema:author" href="https://github.com/StefanCosminR" typeof="schema:Person">
                        <span property="schema:givenName">Stefan-Cosmin</span>
                        <span property="schema:familyName">Romanescu</span>
                    </a>
                </li>
                <li class="inline-list-item" property="schema:author" typeof="sa:ContributorRole">
                    <a property="schema:author" href="https://github.com/DragosDorneanu" typeof="schema:Person">
                        <span property="schema:givenName">Dragos-Andrei</span>
                        <span property="schema:familyName">Dorneanu</span>
                    </a>
                </li>
            </ol>
        </section>

        <section typeof="sa:Abstract">
            <h2>Abstract</h2>
            <p>
                Develop a multi-device (micro-)service-oriented system able to model and manage, in a smart way, public
                technical content regarding Web development (such as tutorials, presentations, examples of source-code,
                news, etc.) from multiple sources such as DevDocs, GitHub Pages, MDN Web Docs, Programmable Web, Reddit,
                and others.
            </p>
            <p>
                Using the Linked Data principles, the application will generate and expose for human users and software
                – via a SPARQL endpoint – knowledge of interest for Web developers according to various criteria: topic
                (e.g., a certain programming language/paradigm, only guides/references about a specific framework),
                target platform, purpose, geographical area, period of time, user preferences, etc.) Additional
                knowledge provided by Wikidata and ACM Computing Classification System will be used.
            </p>
        </section>

        <section typeof="sa:MaterialsAndMethods">
            <h2>Motivation</h2>
            <p>
                Information about software development is readily available on the web today, unfortunately it is mostly
                simply data, understandable by humans in the right context but very hard to aggregate and search by
                computers. SATO makes a step forward in this direction and comes up with a solution to unite big
                software development resources (such as GitHub, MDN, DevDocs etc.) under one mashup API adding
                semantics to data by making them available in the RDF format.
            </p>
        </section>

        <section>
            <h2>The service architecture</h2>
            <figure typeof="sa:Image">
                <img src="./WADE/SATO-Architecture.png" alt="Service Architecture">
                <figcaption>Fig. 1 - Service architecture</figcaption>
            </figure>
            <section>
                <h3>General Overview</h3>
                <p>
                    The SATO service promotes an architecture based on micro-services so that it can scale well and
                    be available 99% of the time. Being a micro-service based application, SATO has multiple loosely
                    coupled and high cohesive components:
                </p>
                <ol>
                    <li>Multiple servers that expose the SATO API</li>
                    <li>SATO servers Load Balancer</li>
                    <li>Data Source Services</li>
                    <li>Data Source SPARQL Databases</li>
                    <li>Users Service</li>
                    <li>Data Refresher Service/Job</li>
                    <li>User Interface Server</li>
                    <li>Content Distribution Network</li>
                    <li>Client Application</li>
                </ol>
                <p>
                    The scalability of SATO service comes from distributing the tasks across multiple resolvers.
                    The resolvers either work in parallel or act as a group responsible for resolving some operations
                    and the workload is distributed across the group.
                </p>
                <p>
                    The entities that work in parallel are the Data Source Services about which details are given
                    in the following subsections. For performance and availability reasons, to avoid cases when
                    there exist a single resolver and has a lot of workload causing it to fail in the end, for the
                    SATO servers an Autoscaling Group is created on which a Load Balancer decides to which instance
                    of the Group to forward the request.
                </p>
                <p>
                    For ensuring that users data is not lost and is easy to query, storage services from
                    <a property="schema:url" href="https://firebase.google.com">Firebase</a> are used. This services
                    assure read and write performance and benefit of a data replication feature typical to Cloud
                    Storage Services that copies the stored information into more locations (default is 3). More over,
                    this feature gives a performance boost because the write operations are by default executed on a
                    replica and the read operations on the other replicas.
                </p>
                <p>
                    The Client Web application load time is boosted by using a Cloud Content Distribution
                    Network for general data caching.
                </p>
            </section>
            <section>
                <h3>Authentication</h3>
                <p>
                    In order to ensure a secure experience for the users of the API, the service uses a well known and
                    tested authentication system provided by <a property="schema:url"
                                                                href="https://firebase.google.com">Firebase</a>.
                    The provided authentication service allows us to have a scalable and easily available
                    authentication API. More over, our service would benefit of the possibility to implement
                    OAuth 2.0 based authentication for our customers to use other already existing accounts from
                    <a property="schema:url" href="https://www.facebook.com/">Facebook</a>,
                    <a property="schema:url" href="https://twitter.com/login">Twitter</a>,
                    <a property="schema:url" href="https://github.com/">Github</a> and others.
                </p>
            </section>
            <section>
                <h3>Authentication (Updates)</h3>
                <p>
                    Given the target goal of the designed system and the typology of the expected users (e.g: technical
                    users, students, academic staff), the most suitable authentication method for all this major user
                    categories is Github.
                </p>
                <p>
                    Github would be used for OAuth authentication via Firebase Auth API and would provide (with user
                    agreement) information about what kind of work they perform on Github. This would help SATO to
                    propose a set of suggested resources that could be of interest for the authenticated user.
                </p>
            </section>
            <section>
                <h3>Data source services</h3>
                <p>
                    The data available about software development is not using a RDF format in most cases.
                    To solve this and to minimize the calls to external APIs we introduced the "Data Source Services".
                </p>
                <p>
                    A "Data Source" is an existing service that offers information to our service (eg: Github,
                    MDN etc.).
                    These type of services have the role to interrogate their corresponding API (GitHub, MDN etc.)
                    on demand, convert the date to RDF and store it in their own triplestore.
                </p>
                <p>
                    In order to have control over the freshness of the data, each service exposes an API endpoint
                    that will be called by a "Data Refresh Job" when it considers it is the best time to update the
                    databases.
                </p>
            </section>
            <section>
                <h3>Data source services (Updates)</h3>
                <p>
                    The purpose of this services remains the same. The only difference is that that implies the
                    system architecture change that will follow to be described in the following major section.
                    All the data source services will collect data and store it in a common replicated triplestore.
                </p>
                <p>
                    Taking into consideration general technical usability, documentation, performance and accessibility,
                    the chosen triplestore is that offered by
                    <a property="schema:url" href="https://www.stardog.com/docs">Stardog</a>.
                </p>
            </section>
            <section>
                <h3>Public facing API</h3>
                <p>
                    In order to access the data collected by the aforementioned services, we expose a microservice that
                    checks for user authentication, accepts SparQL queries, offers some predefined queries based on the
                    user preferences and aggregates the query result from all data source services.
                </p>
            </section>
            <section>
                <h3>Public facing API (Updates)</h3>
                <p>
                    The exposed microservice, provides the external users and systems with an embedded SparkQL endpoint.
                    This allows SATO system to accept any correctly formatted SparQL query and to retrieve the results
                    in many well know formats such as application/rdf+xml and application/ld+json.
                </p>
                <p>
                    The user authentication is not a restriction anymore. Any user can benefit of resources from our
                    knowledge graph. The authenticated users have the advantage of having suggested a list of possible
                    resources of interest to them according to their profile (Content based profiling based on Github
                    information).
                </p>
                <p>
                    The data is already aggregated in the knowledge graph. The SparQL endpoint must know how to take
                    the data from the database and how to send the data back to the user. The aggregation at SparQL
                    endpoint level is not needed anymore.
                </p>
            </section>
        </section>

        <section>
            <h2>The service architecture (Updates)</h2>
            <figure typeof="sa:Image">
                <img src="./WADE/newDiagrams/architecture.png" alt="Updated Service Architecture">
                <figcaption>Fig. 1' - Updated Service architecture</figcaption>
            </figure>
            <section>
                <h3>General Overview</h3>
                <p>
                    The initial proposed architecture was based on multiple SparQL Databases that each correspond to a
                    single Data Source Service.
                </p>
                <p>
                    Given the fact that most of the information that is aggregated within the system has some similar
                    properties, it is only natural to consider having this common properties in the same knowledge
                    graph. This idea leads us to the possibility of linking entities from one data source (e.g: Github)
                    to entities from another data source (e.g: Reddit) in order to create statistics and derive new
                    possible suggestions for our clients.
                </p>
                <p>
                    More over, the new cluster of replicated knowledge graphs helps in simplifying the implementation
                    of the exposed SparQL endpoint. By keeping all the data in the same replicated knowledge graph,
                    no aggregation business logic is required to be implemented at SparQL endpoint level in order to
                    handle different transformations to send the data back to the client in the expected format.
                </p>
                <p>
                    One example of statistic that can be computed by harnessing the properties of the build knowledge
                    graph would be that of analysing the collaboration to a topic from users across all the Data Sources
                    that are integrated with the built system.
                </p>
                <p>
                    The performance and scalability considerations were not neglected by the necessity of combining our
                    data sources into the same knowledge graph. In this direction, a SparQL Databases cluster would be
                    created. Each of the instances in the cluster would hold a replica of the knowledge graph and
                    requests would be balanced across all of them for high availability and assuring the information
                    safeness.
                </p>
            </section>
        </section>

        <section>
            <h2>SATO Ontology</h2>
            <figure typeof="sa:Image">
                <img src="WADE/newDiagrams/OntologyDiagram.png" alt="SATO Ontology">
                <figcaption>Fig. 2 - SATO Ontology</figcaption>
            </figure>
            <section>
                <h3>General overview</h3>
                <p>
                    The SATO Ontology identifies common properties and features of data sources such as Github, DevDocs
                    and Reddit.
                </p>
                <p>
                    The blue rectangles represent common classes of objects that can be reused to describe information
                    from multiple data sources. For example, DevDocs data source can be described only using this common
                    classes.
                </p>
                <p>
                    The orange rectangles describe Github classes and their relationship with each other and the common,
                    blue classes.
                </p>
                <p>
                    The red rectangles are used to define classes extensions for Reddit where the concept of Author,
                    Article and Tutorial must be defined.
                </p>
                <p>
                    This ontology is defined so that any type of Web resource can be defined, not only those related to
                    web development.
                </p>
            </section>
        </section>

        <section>
            <h2>Module architecture</h2>
            <figure typeof="sa:Image">
                <img src="./WADE/SATO-Main-Modules.png" alt="Module Architecture">
                <figcaption>Fig. 3 - Module architecture</figcaption>
            </figure>
            <section>
                <h3>General overview</h3>
                <p>
                    The application is composed from multiple reusable modules that are common between some application
                    components. The most notable and important modules which are detailed in the following subsections
                    are:
                </p>
                <ol>
                    <li>Data Source Adapters Module</li>
                    <li>Data Transformation Module</li>
                    <li>Data Persistence Module</li>
                    <li>Models Module</li>
                </ol>
            </section>
            <section>
                <h3>Data Source Adapters Module</h3>
                <p>
                    The application is based on the Data Source Services which are responsible for collecting relevant
                    information about different topics.
                </p>
                <p>
                    Each of those services will use an Adapter object that corresponds to a single Data Source. The
                    Adapter object will wrap the API calls to the Data Source with business logic necessary to
                    validate the API input, process and store the gathered information in a triplestore.
                </p>
            </section>
            <section>
                <h3>Data Transformation Module</h3>
                <p>
                    When refreshing the triplestore information, each Adapter will have to transform the collected
                    data from the format it was received in to a RDF format accepted by the triplestore where
                    the information will be saved.
                </p>
                <p>
                    To decouple the data transformation logic from the storing logic, each Data Source Adapter will
                    know the correct Data Transformer for his Data Source response and pass it to the Data Persistence
                    Module where the transformation will be called and the transformation result will be stored.
                </p>
            </section>
            <section>
                <h3>Data Persistence Module</h3>
                <p>
                    The Data Persistence Module implements the business logic to correctly store the data collected by
                    Data Source Adapters. When calling the data store logic, a Data Transformer will be called to
                    transform the data in a triplestore compatible format that will be saved.
                </p>
                <p>
                    The Adapters know how to use the Data Persistence Module and the Data Persistence Module knows how
                    to use the Data Transformers passed by the Adapters in order to transform and persist the data.
                </p>
            </section>
            <section>
                <h3>Models Module</h3>
                <p>
                    This module contains simple classes that define concepts, entities, input and output formats.
                    It is a common module used across the application in contexts where building an input object,
                    an output object or any Object Oriented representation of an information is required.
                </p>
            </section>
        </section>

        <section>
            <h2>Main Application Features</h2>
            <section>
                <h3>Query for information</h3>
                <figure typeof="sa:Image">
                    <img src="./WADE/QueryForTopics.png" alt="Query">
                    <figcaption>Fig. 4 - Search for information regarding some topics</figcaption>
                </figure>
                <p>
                    From the Client application, an AJAX call will be made to collect information about some specific
                    topics.
                </p>
                <p>
                    This request will go through the Load Balancer that stands before the servers that each
                    expose the SATO API. The Load Balancer chooses to proxy the request to one of the available
                    server instances.
                </p>
                <p>
                    The chosen server will dispatch a SparQL query based on the searched topics to each of the
                    Data Source SPARQL databases. Pages from the selected information will be aggregated into the
                    Client response.
                </p>
                <p>
                    Update: The chosen server will dispatch the SparQL query to one of the Stardog instances from within
                    the deployed cluster in order to perform the query. The number of entries in the response can vary
                    according to the SparQL query that was built from users input.
                </p>
                <figure typeof="sa:Image">
                    <img src="./WADE/newDiagrams/QueryForTopics.png" alt="Query">
                    <figcaption>Fig. 4' - Search for information regarding some topics (Updates)</figcaption>
                </figure>
            </section>

            <section>
                <h3>Refresh Data in Triplestores</h3>
                <figure typeof="sa:Image">
                    <img src="./WADE/DataRefreshFlow.png" alt="Query">
                    <figcaption>Fig. 5 - Refresh data stored in each triplestore</figcaption>
                </figure>
                <p>
                    The Refresh Data Job is a periodic operation that assures the freshness of the information stored
                    in the application triplestores. The Refresh Data Job is composed from two main phases:
                </p>
                <ol>
                    <li>Wait for new refresh</li>
                    <li>
                        Trigger Data Source Services Refresh
                        <ul>
                            <li>Emit a refresh request to all Data Source Services</li>
                            <li>
                                The request will make the Data Source Services to collect new information from
                                their corresponding Data Source and persist it into the triplestore
                            </li>
                        </ul>
                    </li>
                </ol>
                <p>
                    After the refresh trigger, the job won't await for any response. It will reenter the first phase
                    and the Data Source Services will perform the update in parallel.
                </p>
            </section>

            <section>
                <h3>Web Resources Suggestions</h3>
                <p>
                    In the context of the client application that SATO offers, for helping users find resources of
                    interest, on the Main Page of the application, there is under the search input a suggestions section
                    where resources that exist in the SATO Knowledge Graph are displayed to users according to different
                    strategies.
                </p>
                <p>
                    At each main page reload, a new set of suggestions is recommended to any user. There are two
                    suggestion strategies: for unauthenticated users and for authenticated users.
                </p>
                <p>
                    For unauthenticated users, a query is made for suggesting random Articles, News and Repositories
                    from the top 100 topics that exist in SATO Knowledge Graph. The query looks like this:
                </p>
                <pre>
            PREFIX : &lthttp://www.semanticweb.org/wade/ontologies/sato#&gt
            PREFIX rdf: &lthttp://www.w3.org/1999/02/22-rdf-syntax-ns#&gt
            SELECT DISTINCT ?url WHERE {
                ?url rdf:type ?type .
                FILTER (?type IN (:Article, :News, :Repository)) .
                ?url :hasTopic ?topic .
                {
                    SELECT ?topic (COUNT(?topic) AS ?occurrences) {
                        ?url :hasTopic ?topic
                    }
                    GROUP BY ?topic
                    ORDER BY DESC(?occurrences)
                    LIMIT 100
                }
            }
            ORDER BY RAND()
            LIMIT 8
                </pre>
                <p>
                    For authenticated users, the suggestions query is influenced by the user interests that were
                    extracted from the Github profile that was used for sign in. The suggestion query for two
                    interests/topics looks like this:
                </p>
                <pre>
            PREFIX : &lthttp://www.semanticweb.org/wade/ontologies/sato#&gt
            PREFIX rdf: &lthttp://www.w3.org/1999/02/22-rdf-syntax-ns#&gt
            SELECT DISTINCT ?url {
                ?url rdf:type ?type .
                FILTER (?type IN (:Article, :News, :Repository)) .
                ?url :hasTopic ?topic .
                FILTER (CONTAINS(STR(?topic), 'topic1') ||
                        CONTAINS(STR(?url), 'topic1') ||
                        CONTAINS(STR(?topic), 'topic2') ||
                        CONTAINS(STR(?url), 'topic2')) .
            }
            ORDER BY RAND()
            LIMIT 8
                </pre>
            </section>

            <section>
                <h3>Advanced Search</h3>
                <p>
                    SATO offers support for advanced search over the SATO Knowledge Graph within the implemented
                    client application.
                </p>
                <p>
                    The supported filtering options are:
                </p>
                <ul>
                    <li>Search by pattern</li>
                    <li>Resource type (Article, News, Repository, Tutorial)</li>
                    <li>Topics of interest</li>
                    <li>Exclude not interesting topics</li>
                    <li>Programming Language</li>
                    <li>Platform (Android, iOS, Linux, Windows)</li>
                    <li>Date Range</li>
                </ul>
                <p>
                    SATO client application automatically builds SparQL queries that apply each of the filters
                    enumerated above. Bellow can be found multiple advanced search query that can be built using
                    the advanced search functionality:
                </p>
                <pre>
# collect first 5 articles or news that are related to android but do not
# refer to C++ or React Native (Java and Kotlin remain alternatives)
PREFIX : &lthttp://www.semanticweb.org/wade/ontologies/sato#&gt
PREFIX rdf: &lthttp://www.w3.org/1999/02/22-rdf-syntax-ns#&gt
PREFIX rdfs: &lthttp://www.w3.org/2000/01/rdf-schema#&gt
SELECT DISTINCT ?url WHERE {
    ?s rdf:type &lthttp://www.semanticweb.org/wade/ontologies/sato#Resource&gt .
    BIND(?s AS ?url) .
    ?s rdf:type ?type .
    FILTER (?type IN (&lthttp://www.semanticweb.org/wade/ontologies/sato#Article&gt,
                      &lthttp://www.semanticweb.org/wade/ontologies/sato#News&gt)) .
    ?s :hasTopic ?topic .
    FILTER (?topic IN (&lthttp://www.semanticweb.org/wade/ontologies/sato#android>,
                       &lthttp://en.wikipedia.org/Android_(operating_system)>) ||
            CONTAINS(STR(?s), 'android') ||
            CONTAINS(STR(?s), 'http://en.wikipedia.org/Android_(operating_system)')) .
    FILTER NOT EXISTS {
        ?s :hasTopic &lthttp://en.wikipedia.org/C++>
    } .
    FILTER NOT EXISTS {
        ?s :hasTopic &lthttp://www.semanticweb.org/wade/ontologies/sato#C++>
    } .
    FILTER NOT EXISTS {
        ?s :hasTopic &lthttp://en.wikipedia.org/React_(web_framework)>
    } .
    FILTER NOT EXISTS {
        ?s :hasTopic &lthttp://www.semanticweb.org/wade/ontologies/sato#react>
    } .
    FILTER NOT EXISTS {
        ?s :hasTopic &lthttp://www.semanticweb.org/wade/ontologies/sato#react-native>
    } .
}
LIMIT 5
                </pre>
                <pre>
# collect all articles that are about linux and windows platform
# that were written after 2019-01-01
PREFIX : &lthttp://www.semanticweb.org/wade/ontologies/sato#>
PREFIX rdf: &lthttp://www.w3.org/1999/02/22-rdf-syntax-ns#> 
PREFIX rdfs: &lthttp://www.w3.org/2000/01/rdf-schema#> 
SELECT DISTINCT ?url WHERE { 
    ?s rdf:type &lthttp://www.semanticweb.org/wade/ontologies/sato#Resource> . 
    BIND(?s AS ?url) . 
    ?s rdf:type ?type . 
    FILTER (?type IN (&lthttp://www.semanticweb.org/wade/ontologies/sato#Article>)) . 
    ?s :hasTopic ?platform . 
    FILTER (?platform IN (&lthttp://www.semanticweb.org/wade/ontologies/sato#linux>, 
                          &lthttp://www.semanticweb.org/wade/ontologies/sato#windows>) 
            || CONTAINS(STR(?s), 'linux') || CONTAINS(STR(?s), 'windows')) . 
    ?s :createdAt ?date . 
    FILTER(?date >= '2019-01-01'^^xsd:date) 
} 
LIMIT 5
                </pre>
                <pre>
# collect articles and news about Web Development that were created in the
# last year (2019)
PREFIX : &lthttp://www.semanticweb.org/wade/ontologies/sato#> 
PREFIX rdf: &lthttp://www.w3.org/1999/02/22-rdf-syntax-ns#> 
PREFIX rdfs: &lthttp://www.w3.org/2000/01/rdf-schema#> 
SELECT DISTINCT ?url WHERE { 
    ?s rdf:type :Resource . 
    BIND(?s AS ?url) . 
    ?s rdf:type ?type . 
    FILTER (?type IN (:Article, :News)) . 
    ?s :hasTopic ?topic . 
    FILTER (?topic IN (:Web_development) || CONTAINS(STR(?s), ':Web_development')) . 
    ?s :createdAt ?date . 
    FILTER(?date >= '2019-01-01'^^xsd:date) 
}
                </pre>
                <pre>
# collect all resources available about Web Frameworks that are not related
# to JavaScript starting 2018
PREFIX : &lthttp://www.semanticweb.org/wade/ontologies/sato#> 
PREFIX rdf: &lthttp://www.w3.org/1999/02/22-rdf-syntax-ns#> 
PREFIX rdfs: &lthttp://www.w3.org/2000/01/rdf-schema#> 
SELECT DISTINCT ?url WHERE { 
    ?s rdf:type :Resource .
    BIND(?s AS ?url) . 
    ?s :hasTopic ?topic . 
    FILTER (
        ?topic IN (:webframework, &lthttp://en.wikipedia.org/Software_framework>,
                   :web-framework)
        || CONTAINS(STR(?s), 'webframework')
        || CONTAINS(STR(?s), 'http://en.wikipedia.org/Software_framework')
        || CONTAINS(STR(?s), 'web-framework')
    ) .
    FILTER NOT EXISTS { 
        ?s :hasTopic &lthttp://en.wikipedia.org/JavaScript> 
    } . 
    FILTER NOT EXISTS { 
        ?s :hasTopic :JavaScript 
    } . 
    FILTER NOT EXISTS { 
        ?s :hasTopic :javascript
    } . 
    ?s :createdAt ?date . 
    FILTER(?date >= '2020-01-12'^^xsd:date)
}
                </pre>
                <pre>
# collect all resources about Big Data that are dated after 2020-01-12
PREFIX : &lthttp://www.semanticweb.org/wade/ontologies/sato#> 
PREFIX rdf: &lthttp://www.w3.org/1999/02/22-rdf-syntax-ns#> 
PREFIX rdfs: &lthttp://www.w3.org/2000/01/rdf-schema#> 
SELECT DISTINCT ?url WHERE { 
    ?s rdf:type :Resource . 
    BIND(?s AS ?url) . 
    ?s :hasTopic ?topic . 
    FILTER (?topic IN (&lthttp://en.wikipedia.org/Big_data>, 
                       &lthttp://en.wikipedia.org/Apache_Hadoop>, 
                       &lthttp://en.wikipedia.org/Apache_Spark>) 
            || CONTAINS(STR(?s), 'http://en.wikipedia.org/Big_data')
            || CONTAINS(STR(?s), 'http://en.wikipedia.org/Apache_Hadoop')
            || CONTAINS(STR(?s), 'http://en.wikipedia.org/Apache_Spark')
    ) . 
    ?s :createdAt ?date . 
    FILTER(?date >= '2020-01-12'^^xsd:date) 
}
                </pre>
            </section>
        </section>

        <section>
            <h2>UML diagram</h2>
            <figure typeof="sa:Image">
                <img src="./WADE/UMLDiagram.png" alt="UML diagram">
                <figcaption>Fig. 6 - UML diagram</figcaption>
            </figure>
            <section>
                <h3>General Overview</h3>
                <p>
                    In this section, the main classes that compose the SATO service components are described and their
                    principal use cases are detailed. The classes hierarchy presented in the above UML diagram are used
                    to compose the Authentication mechanism and the Data Source Services.
                </p>
            </section>
            <section>
                <h3>Authentication Service</h3>
                <p>
                    The authentication service has the main purpose of being a facade to the Users database,
                    as such its classes are modeled to resemble the database.
                </p>
                <p>
                    An important interface for this service is called "<b>Identifiable</b>". It contains a property ID
                    that helps in uniquely identifying model instances.
                </p>
                <p>
                    Since databases need to have a primary key on each table, this interface is used by a vast number
                    of objects that map directly to the database. It is useful for creating a homogeneous base
                    structure for the SATO concepts that must be queried and processed. Also, having this interface
                    helps in easy building of an in memory cache by using an efficient data structure for CRUD
                    operations such as a HashMap instead of an Array.
                </p>
                <p>
                    The main class in this module is the "<b>User</b>" class. This contains only a userName and password
                    for users that chose to use this form of authentication instead of OAuth 2.0 services.
                    Another important field is "<b>interests</b>" which is an array of all the interests of a user that
                    helps in composing personalized content. The interests are instances of class "<b>Topic</b>" that
                    holds an URL to the corresponding topic schema. The schema URL is used for improving the quality of
                    the knowledge graph of user interests that SATO service uses.
                </p>
            </section>
            <section>
                <h3>Data Source Service</h3>
                <p>The data source services use two important interfaces:</p>
                <ol>
                    <li>
                        <b>DataSource</b>
                        <ul>
                            <li>Holds an URL to a data source</li>
                            <li>
                                Each specific service will create a class implementing this interface which will
                                provide the logic for querying its destined data source
                            </li>
                        </ul>
                    </li>
                    <li>
                        <b>Transformer</b>
                        <ul>
                            <li>Exposes a single method, called "transform"</li>
                            <li>
                                This method will take as input the raw data collected from a Data Source and it will
                                return it in the RDF format that will be persisted
                            </li>
                            <li>
                                As in the previous case, each service will implement this interface and pass it to a
                                class that will know how to use the transformer and to store the result.
                            </li>
                        </ul>
                    </li>
                </ol>
                <p>
                    Another important component is the "<b>DataCollector</b>" class. This takes as input a data source
                    and a transformer, applies the transformation on the data received by the data source and it will
                    use a connection to a Triplestore to write the obtained RDF Data.
                </p>
            </section>
        </section>

        <section>
            <h2>External knowledge and interesting queries</h2>
            <p></p>
        </section>

        <section>
            <h2>Bibliografie</h2>
            <ol class="centered-list">
                <li><a target="blank" href="https://www.stardog.com/docs">Stardog</a></li>
                <li><a target=blank href="https://jena.apache.org/getting_started/index.html">Apache Jena</a></li>
                <li><a target=blank href="https://docs.docker.com">Docker</a></li>
            </ol>
        </section>
    </article>
</fieldset>
<footer>
    <p>
        Project for <a href="https://profs.info.uaic.ro/~busaco/teach/courses/wade/">WADE</a>
        &middot;
        <a href="http://www.info.uaic.ro/bin/Main/">@UAIC Iasi</a>
    </p>
</footer>
</body>
</html>
